As pointed out earlier, the consistency of data stored in NVRAM is vulnerable to crashes or power failures. Since NVRAM is directly attached to the processor memory interface, there is no need to use techniques such as DMA to transfer a modified page to external storage. This also means that a memory operation solely relies on the CPU which usually gives no confirmation when that operation completes. In this context, there are two major issues that threaten the consistency of data written to NVRAM, namely out-of-order execution and deferred write-back.

### Out-Of-Order Execution

In an attempt to optimize instruction throughput, processors may reorder instructions at runtime. This enables processors to optimize resource utilization and hide latencies of time-consuming instructions. However, only reorderings that do not violate data dependencies between instructions are possible. While processors do prevent such conflicts, there are dependencies that cannot be observed. For example, in order to mark a chunk of data as durable in NVRAM, one might store a designated token immediately after the operation completed. With out-of-order execution it is possible that the token is written before the payload. This can lead to severe inconsistencies especially when a crash prevents the chunk from being written.

A common method to counter this issue is to enforce program order with memory barriers (also fences). A memory barrier prevents the CPU from proceeding until all prior memory operations have completed. Although a barrier does not directly order its preceding instructions, it can be used to impose an order on separate sequences of instructions. An example for a memory barrier is SFENCE on x86 architectures. While this approach solves the initial problem, it has a notable drawback. Barriers defeat the purpose of out-of-order execution. As a result, CPU pipelines are likely to stall, hence reducing resource utilization. Therefore, barriers can have significant impact on runtime performance, unless used judiciously.

### Deferred Write-Back

In many modern processor architectures store operations may not  be immediately lead to an update in main memory. This behaviour can be caused by intermediary buffers such as memory order buffers, caches, and memory controller buffers. While their individual purpose may vary, they all defer memory write-back operations. This is a known vulnerability for consistency in NVRAM as the mentioned buffers are volatile and deferred stores may be lost when power fails {condit2009better, oukid2017data}. In order to preserve consistency, it is necessary to force write-back in all of these cases.

In conjunction with instruction scheduling and cache coherency protocols a memory order buffer may be present. It holds all loads and stores, with the exception of non-temporal operations.  In order to prevent a deferred write-back through MOB, its store buffer must be flushed. On x86 architectures this can be achieved with a store fence operation such as SFENCE {intel2017sdm}.

Processor caches help avoid access latencies and reduce memory bus traffic for frequently used data. A possible exception are non-temporal stores and data chunks marked as uncacheable. As well as memory order buffers caches are volatile, so an abrupt power failure may lead to lost updates. The issue with this is not that updates are lost but that it is unclear which updates are lost if any. The reason for this circumstance is the cache eviction policy trying to compensate for typically narrow cache volume. Depending on policy, cache content, and system load a modified chunk may or may not be flushed to main memory. An example of an application scenario where such behaviour is unacceptable is transactions. Imagine an updating transaction t1 that commits but is not evicted from cache. Then a transaction t2 based on the result of t1 commits and gets evicted from cache. If the system crashes at this point, t2's result will be durable while t1's initial update is lost.

T1: r(A) w(A) c*   -- Cache ---------------------- (crash)
T2: -------------- r(A) w(B) c -- Cache -- RAM --- (crash)

An approach to prevent such inconsistencies is to mark relevant memory pages as uncacheable but that could introduce considerable overhead for frequently used data. A more popular approach is to evict cache lines programmatically whenever necessary {?}. On x86 architectures this can be done with the CLFLUSH or CLWB instructions {intel2017sdm}. However, the problem with a simple cache line flush such as CLFLUSH is that making a cache line durable may not always mean that is should be evicted. In this regared, CLWB compensates for this matter by retaining the designated cache line.

Once a cache line is flushed, it is propagated to the memory controller where it is buffered in a write-back queue before being written onto the device. Again, the problem is that such a buffer is usually volatile. This means that a power failure could lead to lost updates to NVRAM. Even though residual power in DRAM has been shown to be substantial, there is no reliable way to ensure a full buffer flush {halderman2008lest}. This circumstance has given rise to many discussions in the past {condit2009, ?}. Some authors proposed a designated instruction for flushing write-back queues. An example is the meanwhile deprecated PCOMMIT {oukid2015instant, schwalb2015nvm_malloc, volos2017whisper}. Others have developed more general mechanisms for preserving consistency in NVRAM that also address this issue {condit2009better, pelley2014memory}. The current state in this regard is that platforms must provide Asynchronous DRAM Refresh which ensures a queue flush whenever a power failure is detected {volos2017whisper}.