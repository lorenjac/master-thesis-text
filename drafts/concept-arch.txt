After outlining goals and preconditions in the previous section, this section presents the architecture of the KVS. It covers all aspects, except crash consistency and concurrency control which are dealt with in more detail in subsequent sections.

The KVS is designed for multi-core architectures and relies on both volatile and non-volatile memory attached to the system memory interface. In order to take advantage of both types of memory, the KVS is designed as a two-level store which only updates NVRAM when a transaction commits. Consistency across crashes is ensured with existing hardware primitives and upcoming platform features. Concurrent transactions are controlled by a serializable variant of SI. Background on these design decisions is given below.

## System Architecture

Designing a runtime-critical software such as databases not only involves knowledge about expected workloads but also about the underlying computing device. While workloads have been discussed earlier, this section describes the system architecture of the intended KVS.

### Concurrency

The KVS is designed for a single-node architecture. Even though distributed databases are fairly common, there seems to be no apparent reason for them to reveal any more insight on leveraging NVRAM for concurrency. Also distributed systems involve much more complex mechanisms such as consensus among distributed transactions, all of which are beyond the scope of this work. However, future work should investigate whether the conclusions of this work also hold for distributed databases.

In order to achieve scalable transaction throughput through concurrency, the target system is a multi-core architecture. That means, the system features one or more processors with multiple cores, where each core may support multiple hardware threads. On such a system, each transaction is executed in the context of a thread which is scheduled and assigned to a processor core by the operating system. Processors usually coordinate their work by communicating via some form of chip interconnect. In order to preserve generality, this work makes no assumptions concerning the nature of such interconnect networks.

### Memory Architecture

Recent research shows that on traditional hardware it is advisable to continue integrating volatile RAM together with NVRAM. The reason is that not all data is meant to be durable which is especially true for NVRAM where crash consistency is linked with considerable overhead. Manufacturing NVRAM is still challenging, especially in terms of access latency and endurance, but it is expected that these issues will be resolved in the near future. Therefore, in an effort to combine the benefits of both technologies, the memory subsystem is required to feature both volatile RAM and NVRAM. In accordance with recent research it is assumed that both kinds of memory can be accessed through the same memory interface. This work assumes a shared memory architecture. That is, processors may have one or more private cache levels but main memory is accessible to all processors. Conceptually, cache coherence is not required but has the advantage that less effort is spent on coordinating concurrent access to shared data.

The KVS is designed to exclusively reside in main memory. All data that is not required across restarts is stored in volatile RAM, whereas all other data are stored in NVRAM. Multiple recent works have demonstrated that NVRAM can be used to build MMDB without conventional non-volatile storage such as hard drives. As a result, ensuring recovery, which has always been an inherent bottleneck of MMDB, can be eliminated. In addition, near-instantaneous restarts become feasible. As a consequence, conventional storage is not part of the concept for this KVS. While such components may very well be present in a system, they are never used to store any data of the KVS other than its binaries. This way, data access incurs no I/O and restarts do not have to fetch data from slower storage devices. In return, candidate systems must provide sufficient NVRAM capacity to hold the entire database.

A disadvantage of this approach is that the size of the database is bounded by the amount of available NVRAM. In contrast, MMDB usually allow for larger data sets by keeping frequently used data in memory, while others are moved to slower mass storage media. However, main memory capacities have been steadily growing and NVRAM capacities are projected to have at least twice the capacity of DRAM. Another drawback is recoverability in case of device failures. Mass storage not only scales better in terms of capacity, but it also supports redundancy through RAID, for instance. With NVRAM, both capacity and scalability are lower, so employing information redundancy may be prohibitively expensive. Without such measures of fault tolerance, however, a single failed NVRAM module may lead to permanent data loss. This issue is not tackled in this thesis and is therefore left for future work.

## Key-Value Store

This section describes the software architecture of the KVS. That includes the operation principle in terms of transactions, storage, and concurrency as well as the general structure.

### Two-Level Store

As mentioned above, the KVS resides entirely in main memory. This enables fast access to all data within the database and makes swapping obsolete. In return, the size of the database is bounded by the total NVRAM capacity. Apart from capacity, operating NVRAM currently exhibits greater access latencies compared to DRAM. As pointed out in Chapter \ref{ch:nvram}, these latencies mainly affect writes and are attributed to both technology parameters and crash consistency measures. This work assumes, that even as technology improves, crash consistency will continue to come a cost.

In an effort to mitigate these issues, the KVS is designed to use volatile RAM in addition to NVRAM. In order to achieve maximum performance, it attempts to exploit the benefits of either technology while limiting the impact of their drawbacks. For that purpose, the KVS employs a two-level store architecture as in \cite{bailey2013exploring}.

In a two-level store architecture, in-flight data from memory accesses may be buffered in an intermediary storage medium. In this case, write operations are buffered in volatile memory until their associated transaction commits. Only when a transaction commits, all its updates are propagated to NVRAM. Otherwise no user data is written to NVRAM. Read operations are not directly affected by the two-level store paradigm. In some cases, however, an implementation may choose to buffer read operations, for instance to determine serialization order.

The aim of the two-level store architecture is to reduce the impact of NVRAM-related latencies. Buffering updates to NVRAM in volatile memory has several advantages. First, updates are only posted to NVRAM when they need to be which can save both latency and memory bandwidth, especially when aborts are frequent. Second, limiting updates to NVRAM to commit phases allows for bulk writes. This way, expensive consistency procedures do not have to be performed repeatedly for a single transaction. Third, updated items in volatile memory can be accessed with lower latencies, which is true for both read and write operations. In the end, buffering updates also aids recovery, as uncommitted data is guaranteed to be lost after a restart.

### Transactions

* type: flat whole transactions; no auto commit
* concurrency: MVCC
* recovery: implementation-defined

In the event an update propagation is interrupted, for instance by a crash, then partial write-backs may become durable. In this case, the KVS must ensure that any torn writes to not harm the consistency of data. Possible solutions for this problem are recovery routines that locate and invalidate corrupted data or designated bitfields that are guaranteed to be set only after the entire payload has been written. The concrete recovery method to be used is implementation-defined.

### Structures