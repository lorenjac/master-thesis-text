\chapter{Non-Volatile RAM}
\label{ch:nvram}

Popular non-volatile storage technologies such as HDD or SSD can only be
operated as block devices and incur high access latencies compared to volatile
DRAM. This led to the development of non-volatile analogies of DRAM, which
however failed to see wide adoption. One reason was that these devices were
either block-oriented or required additional peripherals such as batteries.

Recent research however, suggests that fast and byte-addressable NVM will be
available in the near future. Based on its projected characteristics, this new
class of memory is also referred to as NVRAM.

\todo[inline]{Short chapter overview}

\paragraph{Terminology}

As of this writing there is no consensus as to how byte-addressable non-volatile
memory should be referred to. This thesis exclusively uses term NVRAM. There are
two reasons for this decision. For one, alternative terms such as BPRAM or PM
suggest that non-volatile memory is also persistent which however, may not
always be correct \todo{proof?}. Another reason is that some terms such as SCM,
NVM and PM may not reflect the property of bytewise addressing which is central
to these technologies. NVRAM on the other hand, explicitly denotes all of the
desired properties.

\section{Applications}

There are numerous examples for applications of NVRAM. While earlier works often
considered NVRAM as a means to improve fault tolerance, recent research suggests
a broader range of applications. This development is especially fuelled by
recent advances in the manufacture of standalone NVRAM instead of battery-backed
DRAM. In practice, application scenarios also include assumptions on the
properties of NVRAM and the surrounding system architecture. Such issues are
covered in later sections.

As pointed out earlier, a well-known use case of NVRAM is to increase fault
tolerance towards crashes. The goal is to retain main memory content even in
case of a crash, for instance by an abrupt power loss \cite{molina1992main}.
This way, critical data such as logs of file systems (also journals) or
databases remain durable and can be used to recover and even complete unfinished
operations such as making a transaction durable \cite{liskov1991replication,
chen1996rio}. Furthermore, NVRAM can be used to create durable disk caches,
hence shrinking the window in which disk IO is vulnerable to crashes
\cite{chen1996rio, wu1994envy}. In the past, such solutions relied on
battery-backed DRAM or SRAM. This was subject to criticism as batteries only
have a limited charge to ensure durability. Also batteries degrade over time and
need to be maintained to prevent unexpected failure \cite{molina1992main}.
Therefore, modern NVRAM solutions which no longer require peripherals are a
welcome improvement in this area.

A significant amount of research on NVRAM is dedicated to mitigating the IO
bottleneck imposed by traditional disk storage. One way to do so is to defer
disk IO via durable disk caches \cite{chen1996rio, wu1994envy}. When an object
on disk is requested it is moved to the disk cache. Once an object is cached it
may be read or modified with accessing the underlying disk. In theory,
write-back is only required when there is not enough space for an incoming cache
item. This way, frequently used objects incur no latency penalty through disk
IO. Many operating systems, such as Linux or BSD implement such mechanisms where
it is usually referred to as page buffer \todo{quote?}. The difference is that
conventional page buffers are volatile and need to be flushed at some point
which requires careful resource management.

Another approach is to treat NVRAM as an equivalent to traditional disk storage.
Early works which were strongly influenced by the lack of high-capacity NVRAM
proposed hybrid storage systems where disk storage was used in conjunction with
NVRAM \cite{wang2002conquest, miller2001hermes}. These works have very similar
assignment policies in that they only store small files such as  metadata or
libraries in NVRAM whereas larger files remain on disk. While this does not
remove disk access as a common bottleneck it certainly alleviates latency for
some frequently accessed files. In this regard, NVRAM-complemented disk storage
systems are similar to those with NVRAM disk caches.

A more rigorous implementation of the paradigm of NVRAM-assisted mass storage is
to remove all traditional storage and rely solely on NVRAM. A prominent use case
for this architecture are MMDB. Conventional MMDB keep all their data in
volatile main memory and only use disks for recovery in case of crash. Recovery
measures such as logging have been a long-standing issue with MMDB as they incur
expensive disk IO, thus limiting the overall throughput. With NVRAM, disk IO can
be eliminated allowing for faster logging and recovery using the main memory
bandwidth. This concept is especially promising as some upcoming variants of
NVRAM are projected to feature larger capacities than conventional DRAM
\todo{quote!}. For this reason, recent research has presented NVRAM-aware
designs for MMDB ranging from complex database systems \cite{oukid2015instant,
schwalb2016hyrise} to key-value stores \cite{bailey2013exploring, zhou2016nvht,
wu2016nvmcached}. Another notable example is \emph{Machine}, a novel computer
architecture which comprises a cluster of special purpose processors and large
amounts of NVRAM \cite{courtland2016can}.

\paragraph{Further Applications}

It was shown in various works that advantages can be gained by using NVRAM.
Still, most proposals assume the presence volatile RAM in addition to NVRAM
\cite{oukid2017data}. The reason behind this assumption is that not all parts of
memory are intended to be durable. This is true for regions capturing machine
state or security-relevant data. Nonetheless, recent research suggests that
systems exclusively based on NVRAM can be built \cite{narayanan2012whole}.
Clearly, such an architecture would have severe consequences for both operating
systems and applications \cite{bailey2011operating}. For example, operating
system processes would remain in memory even if terminated. On the one hand,
this could significantly accelerate the procedure of invoking a process. On the
other hand, all data belonging to a process' address space would be durable even
if they were corrupted by a crash. Other issues are concerned about memory
management, device drivers, and vital information. An early prototype of such a
system is currently in development \cite{courtland2016can}. This topic however,
is beyond the scope of this work and is not subject to further discussion.

In a more recent effort to aid machine learning, it has been proposed to
implement artificial neural networks by means of NVRAM
\cite{fumarola2016accelerating}. ANN perform a weighted sum over all inputs
while an activation function acts as a classifier. For this to work, ANN need to
be trained by properly adjusting the scalar weights. It has been suggested that
this process could be performed directly in NVRAM where updated weights would be
durable without the need for write-back.

\section{Technologies}

In the past, there have been multiple attempts to produce non-volatile
equivalents of main memory. One way is to emulate non-volatility by providing
backup power supplies or combining volatile DRAM and conventional non-volatile
storage in a single module. Another promising approach is to develop alternative
memory techniques that provide the features of NVRAM. This section presents
notable developments in the field of NVRAM.

\paragraph{Early Approaches}

Initially, the intention was to make systems more tolerant to failures, although
opportunities for storage systems were also discussed \cite{molina1992main,
wang2002conquest}. The main issue with DRAM is that, in order to retain its
data, it needs to refresh its cells which requires a power supply. Therefore,
early non-volatile main memories were equipped with batteries or UPS which
allowed to hold information for a limited time in case of a power failure. An
example is the distributed file system \emph{Harp} which assumes battery-backed
DRAM on each node to protect its otherwise volatile log from power failures
\cite{liskov1991replication}. The file cache \emph{Rio} attempts to make
operating systems more resilient to failures by caching files in fast
battery-backed DRAM thus providing a smaller window of vulnerability
\cite{chen1996rio}. Conversely, \emph{Conquest} is a file system that, with the
exception of large files, stores all data including metadata in battery-backed
DRAM \cite{wang2002conquest} to enable less copying and achieve better access latencies.

% Notable examples are the file systems \emph{Harp} \cite{liskov1991replication}
% and \emph{Conquest} \cite{wang2002conquest} as well as the file cache
% \emph{Rio} \cite{chen1996rio}.

In another approach, researchers used battery-backed SRAM as a write buffer
which is flushed to an interconnected flash memory when full or in case of a
power failure \cite{wu1994envy}. By limiting access to the SRAM, the
non-volatile yet slower flash device is effectively shadowed. Still, since flash
operates in block mode, the byte-addressable nature of SRAM cannot be exploited
in this setup. This is an early example of hybrid memory solutions for fast mass
storage. However, backup power supplies have also been subject to criticism.
Arguments are that batteries are not inherently reliable and introduce
additional maintenance overhead \cite{molina1992main}. Therefore experiments
were conducted, where flash memory was directly attached to the memory interface
\cite{shi2010write}. Similar ideas were later consolidated in the JEDEC NVDIMM
specification, which defines several configurations for DIMMs consisting of
flash memory and DRAM \cite{oe2016feasibility, huang2014design}.

% This marks a shift in the development of NVRAM as reliability becomes second
% to fast high-capacity storage.

\paragraph{Modern Approaches} % TODO modern approaches

Early approaches for practical NVRAM were focused on making volatile DRAM retain its information across power outages. There are however promising alternatives such as PCM, MRAM or RRAM which are both byte-addressable and non-volatile by design. Although their underlying principles have been known for a while, further research was required to reach practical designs for manufacturing. Recent research suggests that these NVM technologies will see broad availability in the near future \todo{cite}.

% technologies
    % PCM (also PRAM)
    %     phase-change memory
    %     based on properties of chalcogenide glasses
    %     discovered in 1955
    %     first prototype in 1969
    % STT-MRAM (also ST[T]-[M]RAM)
    %     advanced type of MRAM
    %         magnetoresistive RAM
    %         similar to magnetic core memory (1955)
    %         GMR effect discovered in 1984
    %         developed since 1995 (Motorola)
    %     spin-transfer torque proposed in 1996
    % RRAM (also ReRam)
    %     resistive RAM
    %     resistive switching discovered in 1967
    %     disputed to be a memristor
    % memristors
    %     ???
    % FeRAM

% density: DRAM < STT-RAM < PCM (higher is better)
% endurance: PCM < STT-RAM < DRAM (higher is better)
% latency: DRAM <= STT-RAM < PCM (lower is better)
% dypower: STT-RAM < DRAM < PCM (lower is better)

\section{System Integration}

% consensus that NVRAM will be attached as NVDIMM to system memory interface
%   disadvantages
%       shared bandwith with DRAM
%       DIMM slots do not scale
%       vulnerability to stray writes
% how will it be accessed
%   direct mapped
%       mapping would somehow have to be deterministic
%       or provide designated function for retrieving non-volatile address space
%   device driver
%       file system (map to virtual memory using mmap)
%           ordinary
%           with DAX (direct memory access) [Intel, oukid2017]
%               circumvents page cache for memory-like storage devices

The latter has led to the NVDIMM specification by JEDEC. It introduces
\texttt{NVDIMM-F} and \texttt{NVDIMM-N} which define how to integrate and
interact with combinations of DRAM and NAND\todo{explain}. In an effort to
support recent NVM technologies such as PCM, JEDEC has announced
\texttt{NVDIMM-P}\cite{jedec2017nvdimm}. Therefore, it is assumed that NVRAM
will be available as traditional DIMM modules which are connected to the CPU
memory interface\cite{condit2009better}.

% NVDIMM is attached to CPU but still accessed as block device in separate address space

In order to benefit from NVRAM, operating systems need to find ways to efficiently manage NVRAM. Similarly
to previous variants of NVDIMM, it is proposed that NVRAM is accessed
through a dedicated device driver \cite{intel2017nvdimm}. Another approach would
be to map NVRAM directly into virtual memory \todo{elaborate!}.

At this stage, there are two ways to access the device: raw device access and
file systems. While raw device access appears unwieldy, file systems provide a
great deal of flexibility. Not only do file systems provide a more suitable
abstraction for user-level applications but they also enable memory-mapped IO
through virtual memory. Therefore, file systems are the currently preferred
method of accessing NVRAM \cite{oukid2017data}. Still, there are a couple
of issues to address.

While traditional file systems will continue to work, they may not be
well-suited for NVRAM as they were designed for external block-oriented
storage. Therefore, several NVRAM-aware file systems such as \texttt{pmfs},
\texttt{bpfs}, and \texttt{scmfs} have been proposed \cite{condit2009better,
wu2011scmfs, dulloor2014system}.

% scmcfs (tao2016, complex file system for nvram)
% aerie (volos2014, kvs-like file system for nvram)
% hermes (millter2001, mram as fast disk cache)

\todo[inline]{explain key features of an NVRAM-aware file system}
\todo[inline]{elaborate on role of page caches in operating systems}
% file systems use operating system page cache
% was used to buffer modified pages to reduce disk IO
% no longer required + breaks store/load semantics

\todo[inline]{show how NVRAM memory regions can be exposed (direct, Linux DAX)}
% nvram-aware file systems provide means to expose memory regions
% in linux/ext4: using DAX => circumvents page cache,
% nvram-aware file systems also proposed to expose actual memory regions for direct memory access

\section{Challenges}

Despite the conceivable advantages of NVRAM, there are also challenges to be
addressed. Although most issues are of practical nature there also are
conceptual concerns.

\paragraph{Unintended Durability}

The key feature of NVRAM is to retain its data across restarts. However, not all
data are necessarily intended to be durable. Notable examples include transient,
confidential and corrupt data.

The former comprises data which may not be valid after a system restart, as is
the case with data related to machine or device state.

Other data such as passwords, encryption keys or decrypted data are usually
confidential and are thus not intended to be durable. Also, it has been shown
that even volatile RAM holds its charge long enough so that a module can be
moved to an attacker's machine \cite{halderman2008lest}. By cooling the module
capacitator discharge can be slowed so that it can be moved to another machine
where its content may be read and parsed. Despite being countered with hardware
scramblers, researchers still managed to obtain critical data such as AES
encryption keys using the same technique \cite{yitbarek2017cold}. Such attacks
could be trivial on NVRAM, as durability is its primary feature
\cite{bailey2011operating}. Of course, confidential data could be overwritten
with zeros after usage, but there is always a possibility that a crash might
prevent such clean up tasks from completing. That said, sensitive data should at
all times remain in volatile memory and be nulled after use.

When an operating system or application behaves in erratic fashion or crashes it
may leave corrupted data in memory. Unless memory is cleared or treated as such,
systems incorporating NVRAM could face durable memory corruptions. The latter
may lead to an unstable system producing even more corrupted data. Although the
same is true for conventional non-volatile memory it is still operated through
the operating system in terms of APIs and volatile page buffers. NVRAM on the
other hand is expected to be connected to the memory bus, enabling unbuffered
access through virtual memory addresses. This makes NVRAM vulnerable to stray
writes \cite{condit2009better, oukid2017data}. However, it is suggested that,
when compared to disk storage, stray writes do not occur significantly more
often in NVRAM \cite{chen1996rio}.

\paragraph{Memory Management}

Operating systems designed on volatile RAM usually build a new virtual memory
address space upon each restart. In order to prevent a range of sidechannel
attacks, address space layout randomization (ASLR) is used, leading to a unique
address space layout for each session. This is not a problem for volatile RAM
since it is expected to lose its data either way. NVRAM however, is based around
allowing objects to be accessable after a restart. Therefore, it is necessary to
provide a mechanism to recover the address space of non-volatile memory regions
or objects stored therein \cite{oukid2014sofort, schwalb2016hyrise}.

Most proposals for this problem assume a dual memory management separating
volatile and non-volatile RAM. It is proposed that non-volatile objects will be
accessed through lightweight zero-copy file systems and will be mapped into
individual process address spaces \cite{dulloor2014system, schwalb2016hyrise}.

\paragraph{Consistency}

A notorious problem with NVRAM is consistency in case of crashes. Volatile RAM
does not have this issue as all data is lost when power supply stops.
Conventional non-volatile storage is not immune to data corruption but is
shielded by several operating system mechanisms such as page caches. NVRAM
however, is likely to be attached to the memory bus of a system. Due to the
resulting load/store semantics, operating systems have little to no control over
memory access to NVRAM. Instead the CPU, which is unaware of any consistency
guarantees, is responsible for performing the memory operations on NVRAM. This
makes way for inconsistencies through torn writes in case of a crash. Due to the
complex nature of this subject further discussion is deferred to the next
section.

\section{Preserving Consistency}

As pointed out earlier, NVRAM is vulnerable to inconsistent data in case of
a crash or power failure. Unlike HDD or SSD, NVRAM is part of
the memory address space and accessed through virtual memory rather than IO
ports. Therefore, all data operations on NVRAM must pass through the memory
hierarchy of the CPU. However, there are two essential mechanisms of modern
processors that can break the consistency of data stored in NVRAM:
\emph{instruction reordering} and \emph{deferred write-back}.

\todo{write-back ordering is also a problem (unordered clflush)}

% However, the memory hierarchy is designed for volatile memory which has weaker
% semantics than non-volatile counterpart. As modern CPU have no way to
% distinguish between both types of memory, NVRAM is treated as if it were
% DRAM.

\paragraph{Instruction Reordering}

\todo[inline]{explain principle}
\todo[inline]{explain example for x86}

In order to optimimize instruction throughput of modern processors, instructions may be reordered. This is usually done in a bilateral approach. While compilers may optimize intructions orders statically, modern processors are able to reorder instructions at runtime. However, reordering is only possible if there are no data dependencies between

\todo[inline]{reasoning?}
\todo[inline]{requirements?}
\todo[inline]{example for failure?}

This leads to the conclusion that, preserving data consistency on NVRAM requires the system or programmer to enforce strict ordering of memory operations.

\paragraph{Deferred Write-Back}

\todo[inline]{explain principle}
\todo[inline]{explain example for x86}

Second, many processor architectures do not immediately execute memory
operations but store them in dedicated store and load buffers, respectively. A
store operation is only executed when its store buffer is full or a fence
operation was issued. During this period the store is completely volatile.

Third, unless configured otherwise, processor caches keep cache lines until they
are either evicted to lower cache levels or written back to memory.

Fourth, once the affected cache line of a store is flushed, the store is
propagated to the memory controller where it is once more buffered and written
onto the device, eventually. During this stage the in-flight data is still
volatile and thus vulnerable to power loss.

This behaviour has two consequences: memory operations may be executed in a
wrong order and in-flight stores may not never the device in case of a crash
or power loss.

Modern processors queue memory operations in small buffers in order to achieve
efficient bulk transfers. On Intel \code{x86} architectures there are separate
buffers for load and store operations, respectively. Such buffers are referred
to as MOB. Whenever a store operation is issued, it is added to an
available store buffer. Once satisfied, the buffer is flushed and the stores are
propagated into the first level data cache.

\subsection{Approaches with Existing Hardware}

With existing hardware
% fences

% caches

Instead of waiting for a cache line to be evicted, it can be flushed using
designated instructions. The \code{x86} instruction set provides multiple
instructions to do so: \code{CLFLUSH}, \code{CLFLUSHOPT}, and \code{CLWB}.
While \code{CLFLUSH}

% write-pending queues

While other levels of the memory hierarchy may be flushed through machine
instructions, there is no such instruction for memory controllers. Therefore,
\code{PCOMMIT} a special instruction to flush memory controller buffers was
proposed. Only recently, this instruction has been deprecated in favor of
ADR. ADR is a platform feature that guarantees that, in case of power
loss, all memory controllers flush their buffers using the remaining power of
the system. It works by exploiting the fact that even in case of a power failure
there is sufficient time and power to flush the write buffers of all memory
controllers. For this to work, the power supply unit has to detect a power
failure and issue a signal to the memory controllers.

\subsection{Alternative Approaches}

\todo[inline]{Epoch Barriers, condit, SOSP 2009}
\todo[inline]{Whole System Persistence, narayanan, 2012}
\todo[inline]{Strand Persistency, pelley, ISCA 2014}
