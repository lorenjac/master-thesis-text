Transactions are a powerful concept that has been adopted in various branches of
computer science. Examples include databases, transactional memory, and
operating systems. With transactions, multiple operations, such as reading or
updating a record, can be grouped into a single unit that succeeds if and only
if neither of its operations fails. Especially in high-performance computing
environments, the utilization of computing resources through concurrent
transactions plays an essential role.

This section introduces the concept of transactions and its properties with
regard to concurrency, in particular.

\paragraph{Definition}

A transaction is a sequence of operations that is treated as single atomic
operation, i.e. it either succeeds if all its suboperations succeed or it fails.
In general, an incomplete or failed transaction must not have any observable
side effects. A transaction \emph{commits} when all its subordinate operations
have completed. Once this process is complete, the transaction is
\emph{committed} and all its side effects, if any, become visible.

In general, the concept of a transaction does not impose any restrictions on the
kind of operation enclosed inside a transaction. That is, apart from primitive
operations such as read or update, transactions may also consist of inner
transactions as well. This concept is known as \emph{nested} transactions
\cite{gray1981transaction}. In contrast, \emph{flat} transactions only permit
primitive operations.

Despite their general nature, nested transactions are not a subject of
discussion in this thesis, for a couple of reasons. First, nesting has been
found useful primarily for distributed transaction systems and transactional
programming models neither of which are within the scope of this work
\cite{moss2006open}. In addition to implementation issues, there are several
semantic models for nesting which further complicates its discussion
\cite{harder1993concurrency, weikum1992concepts}. In the end, nesting has not
found wide adoption with the prominent exception of transactional memory
\cite{moss2006nested, moravan2006supporting, saha2006mcrt,
jacobi2012transactional} and a few databases \cite{olson1999berkeley}. Hence,
unless stated otherwise, the term transaction always refers to flat
transactions.

Transactions are useful when a series of operations must either execute in
conjunction or not at all. A simple example is the transfer between bank
accounts. The action of withdrawing a value from one account and depositing it
on another comprises two separate actions that must both be successful in order
to take effect.

% Still, as shown later, even single operations can benefit from
% transactional semantics.

\todo[inline]{Define \textbf{schedule} somewhere}

\paragraph{Transactional Semantics}

The previous definition of transactions was of rather intuitive nature. However,
in order to be useful the semantics of a transaction need to be described more
precisely. The predominant characterization of transactional semantics is ACID
\cite{gray1981transaction, haerder1983principles}. It comprises a set of necessary properties:

\begin{itemize}
    \item atomicity
    \item consistency
    \item isolation
    \item durability
\end{itemize}

Atomicity captures the all-or-nothing notion of a transaction, i.e. either all
operations in its context succeed or none. As a consequence, any already
completed operation of a transaction must be undone should the latter fail.
Reverting the affected data to their previous state is often referred to as
\emph{rollback}.

The property of consistency asserts that if the underlying data are in a
consistent state, then any transaction must preserve consistency. For example,
an ACID-compliant database cannot be transitioned into an illegal state by means
of a transaction. If  a transaction is bound to break the consistency of the
database, then it has to be aborted and rolled back.

In case multiple transactions are executed concurrently, each transaction could
observe intermediate side effects of other concurrent transactions. In order to
prevent this scenario and ensure the consistency property, isolation precludes
transactions from seeing any concurrent activity. The property of isolation is
later dealt with in more detail.

The last of the four ACID properties is durability. It ensures that all side
effects incurred by a committed transaction must be durable across any
subsequent system failure. Durability can be very hard to enforce, especially in
the face of catastropic failures with failing backup media. Therefore its notion
is often relaxed to a reasonable extent.

The ACID criteria have become the prevalent reference for characterizing
transactional systems. However, not all systems enforce the complete set of
properties. Notable examples are transactional memory for conventional RAM and
some cache-like databases which do not support durability as they are volatile
by design. A prominent example for relaxation that is also fundamental to this thesis is the isolation of concurrent transactions.

\paragraph{Serial Transactions}

In a serial transaction-processing system all transactions are executed in a
sequential order. That is, only one transaction, if any, is being executed at a
time and overlapping is not possible. If a transaction $t_1$ attempts to start
while another transaction $t_0$ is still active then $t_1$ has to wait until
$t_0$ terminates.

This reveals two important drawbacks. First, transaction throughput does not
scale as the number of overlapping transaction requests increases. Second, with
only one computing resource active at a time, resource utilization is low on
multi-core architectures. The same is true on single-core systems as execution
latencies cannot be hidden by switching to other transactions. To mitigate these
issues, transactions can be allowed to run concurrently.

\paragraph{Concurrent Transactions}

Concurrent transaction execution can largely remove the shortcomings outlined above. Now, an incoming transaction does not need to wait for an in-flight transaction to complete. In addition to increasing transaction throughput it also enables better resource utilization. This works as long as data are read but not written. Allowing concurrent updates however, bears potential conflicts that threaten the consistency of data and must therefore be addressed. Possible conflicts are:

\begin{itemize}
    \item write-write
    \item write-read
    \item read-write
\end{itemize}

When a transaction $t_1$ attempts to update a record $A$ that was previously
written but not committed by another transaction $t_2$ then $t_1$'s update could
overwrite $t_2$'s update to $A$ before it has become visible. Unaware of the
condition $t_2$ will successfully commit even though its update is lost. This
situation is called a \emph{write-write} conflict.

\begin{lstlisting}
t1: -------w(A)-commit---------
t2: --w(A)---- ... ----commit-- (update is lost)
\end{lstlisting}

In a \emph{write-read} conflict occurs when a transaction reads data that has
not been committed yet. Imagine a transaction $t_1$ that reads a record $A$ that
was previously updated but not committed by another transaction $t_2$. If $t_2$
updates the same item again or fails, then $t_1$ has read a value that was never
committed. This situation is also called \emph{dirty read}.

\begin{lstlisting}
t1: -------r(A)-w(B)-commit-- (B is inconsistent to committed A)
t2: --w(A)--- !!! ----------- (update to A was rolled back)
\end{lstlisting}

The last conflict is called \emph{read-write} conflict and denotes a situation
when a transaction updates a record that was previously read by another
transaction that is still running. Consider two transactions $t_1$, $t_2$ where
$t_1$ reads a record $A$ which is later updated by $t_2$ before either
transaction commits. If $t_1$ reads $A$ again, then the result may be
inconsistent with the earlier read. The situation is also referred to as
\emph{inconsistent read} or \emph{non-repeatable read}.

\begin{lstlisting}
t1: --r(A)--- ... ---r(A)--------- (has read inc. values of A)
t2: ----------w(A)--------commit--
\end{lstlisting}

It is important to note that the conflicts explained above are not precluded by protecting individual read or update operations.

\todo[inline]{Explain consistency in transactional sense vs. race conditions}

With regard to data integrity it is imperative to ensure isolation by preventing these conflicts.

\paragraph{Serializability}

A core concept to preserve consistency in the presence of concurrent
transactions is \emph{serializability}. It is based on the observation that in
an ACID-compliant serial transaction processing system, every sequence of
transactions always yields consistent data. Likewise, a schedule of concurrent
transactions should yield consistent data if and only if it behaves in a way
that is equivalent to a serial sequence of the same transactions. More
precisely, if and only if a concurrent schedule produces the same output as a
serial schedule would have then there are no inconsistencies. This leads to the
formal definition of serializability.

A concurrent schedule is called \emph{serializable} if and only if there exists
a serial schedule of the same transactions that produces the same output. A
transaction processing system provides serializability if and only if it
guarantees that all concurrent schedules are serializable.

\todo[inline]{Examples}

In order to enforce serializability, a decidable classification for serial
transaction schedules is required. A naive approach would be to search for
equivalent serial schedules whenever needed. However, the complexity of
searching for such a schedule grows exponentially, thus making this approach
infeasible. Therefore, more pragmatic approaches were designed.

An early definition of serializability was given in ANSI SQL
\cite{berenson1995critique}. The idea is to identify and detect \emph{anomalies}
of non-serializable schedules at runtime. If an anomaly is detected then any
affected transaction must fail. Based on whether these anomalies were permitted,
several \emph{isolation levels} were defined. In terms of ANSI SQL, a
transaction was serializable if none of the following anomalies was present:

\begin{itemize}
    \item Dirty Read
    \item Non-Repeatable Read
    \item Phantom
\end{itemize}

A \emph{phantom} is similar to a non-repeatable read but differs in that the
item in question is not modified but added or removed. Imagine a transaction
$t_1$ making a conditional selection of items. If another transaction $t_2$ adds
an item and $t_1$ repeats its selection then the result may contain the item
which is inconsistent with the first result.

Note that this formalization is built around the observable artifacts of
non-serializable schedules, rather than their cause such as read-write
conflicts. While it is pragmatic to address only observable anomalies it also
unreliable as more complicated consistencies may remain undetected. In fact, it
was later found that the above characterization is insufficient as further
anomalies were discovered \cite{berenson1995critique, fekete2004read}. In the
wake of these findings, additional restrictions were imposed on the notion of
serializability.

Nevertheless, all of the discovered anomalies can be attributed to the access
conflicts shown above. For example, the most recently discovered anomalies,
\emph{write skew} and \emph{non-serializable read-only} are essentially results
of read-write conflicts. In this sense, a transaction is serializable if and
only if all possible conflicts are precluded. This characterization has several
advantages. Most importantly, it is more plausible to discuss non-serializable
schedules in terms of causes instead of effects. In addition, as opposed to
anomalies the number of conflicts is smaller and also fixed. Therefore, this
thesis primariliy defines serializability in terms of conflicts.

\todo[inline]{Stress importance of isolation/serializability}

\paragraph{Transaction Models}

\todo[inline]{distinguish software tx vs. transactional memory}
