\chapter{Key-Value Stores}
\label{ch:kvs}

A prominent use case for NVRAM are main-memory databases. Even though main
memory and processor caches have become more affordable, MMDB still suffer from
recovery on slower disk drives \cite{oukid2015instant, schwalb2016hyrise}. NVRAM
on the other hand, provides an opportunity to eliminate recovery altogether. An
important class of databases often implemented as MMDB is KVS. Due to their
simplicity and low overhead, KVS have been adopted both in big-data computing
and database research \cite{decandia2007dynamo, lakshman2010cassandra,
wang2015hydradb}. In recent works, KVS were used to explore database design for
NVRAM \cite{bailey2013exploring, zhou2016nvht, wu2016nvmcached}.

This chapter provides a domain analysis of KVS. First, a brief overview of KVS
is given. The aim of this work is to exploit NVRAM for a KVS with fast
conflict-free concurrent transactions. Therefore, a substantial part of the
remaining chapter is dedicated to transactions and concurrency control. The
chapter is concluded with an examination of existing KVS for NVRAM.

\section{Overview}

KVS form an integral part in modern database technology \cite{fiebig2016one}.
This section gives an overview of their properties, classes, and applications.
Compared to other types of databases, KVS are very simple databases that are
sometimes better described by what they are not or do not provide.

\begin{itemize}
    \item non-relational data model
    \item no data schemas
    \item no query languages
\end{itemize}

In general, a KVS consists of a single associative container, where each key is
mapped to exactly one value. A key is an arbitrary string with possible
restrictions on its length. In terms of relational databases, KVS comprise a
single table of two columns. As a result, much of the structural complexity
adherent to relational DBMS is omitted, thus making way for profound
optimization and better response times. Common data structures for associativity
in KVS are hash tables and search trees, in particular B-trees.

Unlike traditional databases, KVS do not impose data schemas. Consequently,
arbitrary chunks of data can be stored as values which is especially useful
in scenarios with no fixed data format or when enforcing one is not a priority.

\begin{lstlisting}
| user | Max                  |
| auth | dxlf240r0g7jr4u8n2oe |
| ...  | ...                  |
\end{lstlisting}

Furthermore, KVS do not provide query languages such as SQL to
store and retrieve data. Instead, KVS are accessed programmatically through a
concise set of operations which is why KVS are also referred to as
\emph{embedded} databases. Although their API is not standardized, it can be
essentially broken down to the following operations.

\begin{itemize}
    \item open/close
    \item insertion
    \item retrieval
    \item removal
\end{itemize}

\paragraph{Applications}

Traditional DBMS are often based on complex architectures featuring query
frontends and sophisticated storage mechanisms. While this works well in many
cases, it severely limits the performance in situations where a simpler storage
paradigm (e.g. key-value pairs) is sufficient. As a consequence, high access
latencies and convoluted, error-prone concurrency schemes inhibit the
scalability of storage systems. KVS on the other hand are designed to compensate
for these shortcomings. A driving force in this regard, are large internet
platforms, e-commerce for instance, and cloud computing services.

A longstanding example of a KVS is BerkeleyDB which acts as a database in a
variety of software solutions. Apart from open-source software such as OpenLDAP
or Apache Web server, BerkeleyDB is also used in a number of proprietary
software such as messaging servers, switches, and routers
\cite{kaestner2007aspect, olson1999berkeley}.

A more recent use case are distributed in-memory caches often found in big-data
environments. Web caches have received great attention as service providers
struggle to scale with rising traffic where many requests target only a small
amount of data \cite{xu2014characterizing}. With caching, a dedicated eviction
policy ensures that \emph{relevant} items reside in memory. As a result, caching
can improve response times significantly. For this purpose, KVS provide an
appropriate abstraction. Important representatives of this class are Redis and
memcached \cite{redis2017home, memcached2017home}. Not only have these KVS been
deployed at companies such as Facebook or Twitter, but they have also formed the
basis for considerable amounts of research in this area
\cite{xu2014characterizing}. Examples include FPGA acceleration
\cite{lavasani2014fpga}, memory partitioning for better cache hit rates
\cite{carra2014memory}, and NVRAM integration \cite{wu2016nvmcached,
malinowski2017using, venkataraman2011consistent}. Still, large companies tend to
maintain in-house solutions to suit their needs \cite{chang2008bigtable,
decandia2007dynamo, lakshman2010cassandra, wang2015hydradb}.

Beyond databases and caches, KVS have also been proposed as a basis for file
systems. In the past, there have been several attempts to integrate database
concepts into file systems, some of which are logging \cite{rosenblum1992design,
tweedie1998journaling} and transactions \cite{seltzer1990transaction,
wright2007extending, spillane2009enabling}. Some studies even suggest that
traditional hierarchical file systems may often be suboptimal
\cite{stein2005stupid, seltzer2009hierarchical}. While databases in general are
still considered too heavy-weight for use in file systems
\cite{seltzer2009hierarchical}, KVS may be a viable alternative. Examples
include the network file system DBFS which is based on BerkeleyDB
\cite{murphy2002design} and FlatFS, a simple file system for NVRAM
\cite{volos2014aerie}. In addition, KVS are also used to complement file
systems, for example, to store metadata as in PVFS \cite{carns2009small}. Still,
the predominant use case of KVS is found in light-weight databases and caches on
top of existing file systems.

\paragraph{Transactions}

An essential feature of most databases is transactions. Transactions enable a
sequence of database operations to appear as a single atomic operation. If a
single operation involved in a transaction fails, the entire transaction fails and its side effects are rolled back.

Transactions are a powerful mechanism that enables aggregated operations without
worrying about inconsistencies even in case of failure. Given the prevalence of
transactions, most KVS support them. Due to their importance for this work,
transactions are covered in more detail in the next section.

\paragraph{In-Memory Operation}

The performance of a database is often denoted in terms of transaction
throughput. One way to increase throughput is to mitigate data access latencies.
Apart from faster storage, this can be done by placing the entire database in
main memory which enables speedups by multiple orders of magnitude. This
approach, which dates back to the mid 1980s has been adopted in many
high-performance databases such as the more recent HANA database
\cite{molina1992main, faerber2012hana}. Likewise, most KVS are explicitly
designed for in-memory operation. Notable exceptions are the popular BerkeleyDB
or Apache's Cassandra where in-memory operation is only an option
\cite{bdb2017doc, lakshman2010cassandra}.

\paragraph{Concurrency}

Another approach to increase transaction throughput is to utilize multi-core
processors by executing transactions concurrently. In order to achieve maximum
performance, it is common for main-memory databases to also support concurrency
\cite{grund2010hyrise, faerber2012hana, diaconu2013hekaton}. Further, it has
been shown that KVS can gain substantial performance benefits through
concurrency \cite{fan2013memc3, li2015architecting, xu2014building}. In fact,
most KVS natively support concurrency with the exception of Redis
\cite{redis2017home}. Unfortunately, concurrency also introduces new issues such
as inconsistencies through race conditions on shared data. Mitigating this issue
can degrade performance which is why many designs trade full consistency against
faster relaxations \cite{decandia2007dynamo}. This issue is dealt with in the
next section about transactions.

\paragraph{Distributed Databases}

As mentioned earlier, KVS play a crucial role in big-data environments. Since
availability is often a requirement in this area, KVS are often implemented as
distributed services \cite{decandia2007dynamo, lakshman2010cassandra,
wang2015hydradb}. Distributed databases and their mechanisms such as distributed
transactions are beyond the scope of this work.

\section{Transactions}

Transactions are a powerful concept that has been adopted in various branches of
computer science. Examples include databases, transactional memory, and
operating systems. With transactions, multiple operations, such as reading or
updating a record, can be grouped into a single unit that succeeds if and only
if neither of its operations fails. Especially in high-performance computing
environments, the utilization of computing resources through concurrent
transactions plays an essential role.

This section introduces the concept of transactions and its properties with
regard to concurrency, in particular.

\paragraph{Definition}

A transaction is a sequence of operations that is treated as single atomic
operation, i.e. it either succeeds if all its suboperations succeed or it fails.
In general, an incomplete or failed transaction must not have any observable
side effects. A transaction \emph{commits} when all its subordinate operations
have completed. Once this process is complete, the transaction is
\emph{committed} and all its side effects, if any, become visible.

In general, the concept of a transaction does not impose any restrictions on the
kind of operation enclosed inside a transaction. That is, apart from primitive
operations such as read or update, transactions may also consist of inner
transactions as well. This concept is known as \emph{nested} transactions
\cite{gray1981transaction}. In contrast, \emph{flat} transactions only permit
primitive operations.

Despite their general nature, nested transactions are not a subject of
discussion in this thesis, for a couple of reasons. First, nesting has been
found useful primarily for distributed transaction systems and transactional
programming models neither of which are within the scope of this work
\cite{moss2006open}. In addition to implementation issues, there are several
semantic models for nesting which further complicates its discussion
\cite{harder1993concurrency, weikum1992concepts}. In the end, nesting has not
found wide adoption with the prominent exception of transactional memory
\cite{moss2006nested, moravan2006supporting, saha2006mcrt,
jacobi2012transactional} and a few databases \cite{olson1999berkeley}. Hence,
unless stated otherwise, the term transaction always refers to flat
transactions.

Transactions are useful when a series of operations must either execute in
conjunction or not at all. A simple example is the transfer between bank
accounts. The action of withdrawing a value from one account and depositing it
on another comprises two separate actions that must both be successful in order
to take effect.

% Still, as shown later, even single operations can benefit from
% transactional semantics.

\paragraph{Transactional Semantics}

The previous definition of transactions was of rather intuitive nature. However,
in order to be useful the semantics of a transaction need to be described more
precisely. The predominant characterization of transactional semantics is ACID
\cite{gray1981transaction, haerder1983principles}. It comprises a set of necessary properties:

\begin{itemize}
    \item atomicity
    \item consistency
    \item isolation
    \item durability
\end{itemize}

Atomicity captures the all-or-nothing notion of a transaction, i.e. either all
operations in its context succeed or none. As a consequence, any already
completed operation of a transaction must be undone should the latter fail.
Reverting the affected data to their previous state is often referred to as
\emph{rollback}.

The property of consistency asserts that if the underlying data are in a
consistent state, then any transaction must preserve consistency. For example,
an ACID-compliant database cannot be transitioned into an illegal state by means
of a transaction. If  a transaction is bound to break the consistency of the
database, then it has to be aborted and rolled back.

In case multiple transactions are executed concurrently, each transaction may
observe intermediate side effects of other concurrent transactions. In order to
prevent this scenario and ensure the consistency property, isolation precludes
transactions from seeing any concurrent activity. The property of isolation is
later dealt with in more detail.

The last of the four ACID properties is durability. It ensures that all side
effects incurred by a committed transaction must be durable across any
subsequent system failure. Durability can be very hard to enforce, especially in
the face of catastropic failures with failing backup media. Therefore its notion
is often relaxed to a reasonable extent.

% The ACID criteria have become the prevalent measure to characterize
% transactionial semantics. Although often associated with ensuring consistency
% for concurrent transactions, ACID is already useful without concurrency. An
% example is atomicity which captures the essence of a transaction even when the
% latter comprises no more than two operations. Moreover, even a transaction that
% consists of a single operation can benefit from ACID-compliance as it guarantees
% that any modifications become durable if and only if the enclosing transaction
% commits. Clearly though, the isolation property is central to preserving
% consistency in the presence of concurrent transactions. In that regard,
% isolation is the equivalent of consistency for concurrency.

The ACID properties have become the prevalent reference for characterizing
transactional systems. However, not all systems enforce the complete set of
properties. Notable examples are transactional memory for conventional RAM and
some cache-like databases which do not support durability as they are volatile
by design. In other cases, guarantees are not dropped but relaxed. As shown
below, a prominent example for relaxation that is also fundamental to this
thesis is the isolation of concurrent transactions.

\paragraph{Concurrent Transactions}

The performance of transaction processing systems such as databases is generally
denoted by their transaction throughput, i.e. the number of transactions
executed in a certain time frame. There are several approaches to increasing
transaction throughput, one being concurrency. In theory, the throughput
improvement on multi-core systems is linear in the number of available cores.
However, there are several factors to consider, some of which are cache
consistency and data integrity. Preserving both can have adverse effects on
performance thus invalidating the above estimation. The following paragraph
deals with data integrity as guaranteed by the isolation property in ACID.

When two or more transactions are run concurrently, race conditions on shared
data are bound to occur. Race conditions on the other hand, may cause data
conflicts which in turn can lead to inconsistent data. The possible conflicts
are:

\begin{itemize}
    \item write-write
    \item write-read
    \item read-write
\end{itemize}


When a transaction $t_1$ attempts to update a record $A$ that was previously
written but not committed by another transaction $t_2$ then $t_1$'s update could
overwrite $t_2$'s update to $A$ before it has become visible. This situation is
called a \emph{write-write} conflict.

\begin{lstlisting}
t1: -------w(A)-commit---------
t2: --w(A)---- ... ----commit-- (update is lost)
\end{lstlisting}

In a \emph{write-read} conflict occurs when a transaction reads data from an
update that has not been committed yet. Imagine a transaction $t_1$ that reads a
record $A$ that was previously updated but not committed by another transaction
$t_2$. If $t_2$ fails for some reason, then $t_1$ could produce inconsistent
data, e.g. by updating another record B based on $A$'s value. This situation is
also called \emph{dirty read}.

\begin{lstlisting}
t1: -------r(A)-w(B)-commit-- (B is inconsistent to committed A)
t2: --w(A)--- !!! ----------- (update to A was rolled back)
\end{lstlisting}

The last conflict is called \emph{read-write} conflict and denotes a situation
when a transaction updates a record that was previously read by another
transaction that is still running. Consider two transactions $t_1$, $t_2$ where
$t_1$ reads a record $A$ which is later updated by $t_2$ before either
transaction commits. If $t_1$ reads $A$ again, then the result may be
inconsistent with the earlier read, hence the term \emph{inconsistent read}.

\begin{lstlisting}
t1: --r(A)--- ... ---r(A)--------- (has read inc. values of A)
t2: ----------w(A)--------commit--
\end{lstlisting}

It is important to note that the conflicts explained above are not precluded by
protecting the individual read or update operations. With regard to data
integrity it is imperative to ensure isolation by preventing these conflicts.

\paragraph{Serializability}

A core concept to preserve consistency in the presence of concurrent
transactions is \emph{serializability}.

\paragraph{Transaction Models}

\todo[inline]{distinguish software tx vs. transactional memory}

\section{Concurrency Control Protocols}

\subsection{Locking}

\subsection{Multiversioning}

\subsection{Serializable MVCC}

\section{Key-Value Stores for NVRAM}

\paragraph{Summary}

\begin{itemize}
    \item KVS are vital database technology; esp. distributed but beyond scope
    \item performance can be increased through in-memory operation + concurrency
    \item a widely-adopted concurrency control is MVCC; but most impl. non-ser.
    \item KVS for NVRAM have shown some potential over traditional KVS
    \item recent KVS for NVRAM all non-ser. => leverage boost to provide ser.
\end{itemize}
