Concurrency is a major building block for scalable transaction processing. It
enables higher transaction throughput and resource utilization compared to
serial processing. On the downside, concurrent schedules are subject to
potential conflicts that may result in data corruption. However, it is not
sufficient to provide mutual exclusion for individual operations within a
transaction. In other words, the scope in which isolation is required spans
beyond critical sections. Therefore, a dedicated concurrency control is required
to ensure isolation. Unfortunately, concurrency controls do not come without overhead which is why, in most cases, a compromise between isolation and performance must be found.

This section deals with concurrency control strategies and outlines the state of
the art with a focus on optimistic approaches, in particular multiversion
concurrency control.

\subsection{Strategies}

There are two fundamental approaches to the design of concurrency controls:
\emph{pessimistic} and \emph{optimistic} protocols \cite{kung1981optimistic,
larson2011high, sadoghi2014reducing}. The distinction is based on whether
conflicts are assumed to be frequent or infrequent. Still, both strategies share
their intent to prevent conflicts from manifesting into inconsistencies.

\paragraph{Pessimistic Concurrency Control}

A pessimistic concurrency control assumes that conflicts are frequent and
strives to prevent conflicts before they can even emerge. To this end,
pessimistic control mechanisms employ some form of exclusive ownership. That
means, a transaction must acquire the ownership of all data items it wishes to
access. If the resource acquisition succeeds, then the transaction can freely
operate on the temporarily owned data. Only when that transaction terminates,
will this ownership be released. If a transaction fails to claim the exclusive
ownership on its data then it has to wait until the required ownership is
granted or abort.

Pessimistic concurrency control is usually implemented with locks. Locking
provides a solid mechanism to ensure serializability and most database systems
implement it \cite{kung1981optimistic, berenson1995critique, larson2011high}.

Despite their prevalence, pessimistic concurrency controls have notable
drawbacks. Locking-based concurrency controls are prone to \emph{deadlocks}
\cite{bernstein1981concurrency, kung1981optimistic}. In order to prevent
deadlocks, they must be detected and resolved which introduces runtime overhead.
Unfortunately, there is no general-purpose locking protocol that precludes
deadlocks \cite{kung1981optimistic}. Another problem is \emph{lock contention}
which occurs when a large portion of concurrent threads in a system compete for
a single shared resource \cite{berenson1995critique, sadoghi2014reducing}. Since
only one transaction can manage to acquire ownership, all remaining transactions
are left waiting for it to complete and contend again. As a result, only one
transaction is executed at a time, thus defeating the purpose of concurrency.

\paragraph{Optimistic Concurrency Control}

Optimistic concurrency controls form the opposite of pessimistic control
schemes. Instead of preventing conflicts altogether, optimistic control schemes
do not enforce consistency until a transaction commits. Only when a transaction
commits, the concurrency control starts to check for violations, a step called
\emph{validation}. If no conflict is detected, then the transaction may commit,
otherwise, it must abort. Validation is crucial for consistency and its
implementation substantially determines the achievable isolation level
\cite{larson2011high}.

Optimistic concurrency control protocols rely on Copy-on-Write (CoW) and
timestamp ordering to synchronize data races of competing transactions
\cite{bernstein1981concurrency, kung1981optimistic}. While readers can access
data without further means of synchronization, writers apply their modifications
only on copies of the original data. Apart from short-duration locks for
critical sections, this approach does not require locking and is therefore not
subject to deadlocks or lock contention. A signature property is that readers
never block which means that both readers and writers never wait for other
readers. An important class of optimistic concurrency controls is
multiversion concurrency control or multiversioning
\cite{bernstein1983multiversion}.

There are, however, disadvantages to this type of concurrency control. While
validation is necessary to ensure that a transaction is not involved in data
conflicts, it can also introduce a significant overhead. First, even when there
are no conflicts, validation is conducted nevertheless. Second, validation
usually requires certain meta data about the operations within a transaction. As
a result, validation complexity scales in size of meta data required to
determine conflict freedom. Another drawback is that aborting a conflicting
transaction means that its entire progress is discarded. In this case, valuable
computing resources have been wasted. This is especially true for update
operations as they involve potentially expensive copy operations.

Despite some drawbacks, optimistic concurrency control has found wide adoption
especially in domains where reads are dominant and conflicts are known to be
infrequent or negligible \cite{carey1986performance, larson2011high,
wu2017empirical}. The scenario of read-dominated workloads has been shown to
apply more often than not \cite{andrei2017sap, wang2017efficiently}. Given its
promising properties, optimistic concurrency control is discussed in more depth
in the subsequent sections.

\subsection{Multiversion Concurrency Control}

MVCC or multiversioning, is an optimistic concurrency control method. Initially,
MVCC was designed as a solution for concurrency control in distributed systems
\cite{reed1978naming}. However, it was also studied in non-distributed settings
and was soon considered a promising alternative to locking
\cite{kung1981optimistic, bernstein1983multiversion, carey1983multiple,
hadzilacos1986algorithmic, carey1986performance}. Subsequently, MVCC has been
adopted in both commercial and non-commercial transaction processing systems,
ranging from general-purpose database systems to high-performance in-memory
databases \cite{larson2011high, lee2013high, diaconu2013hekaton,
schwalb2015efficient}. More recent examples include prototypes of MMDB and KVS
for NVRAM \cite{bailey2013exploring, zhou2016nvht, oukid2014sofort,
schwalb2016hyrise}.

\subsubsection{Principle}

A \emph{version} is a snapshot of a particular data item within a database. In
terms of KVS, that would be a value. In traditional concurrency schemes, there
is exactly one version of each item. These are also referred to as
\emph{single-version concurrency controls}. If a transaction issues an update to
a version, then it is performed in-place. In order to ensure isolation though, a
transaction has to be protected against concurrent reading or writing. This is
usually achieved by having each transaction acquire locks on its data which are
only released once the transaction terminates. This approach is very effective
in that it can provide the highest isolation level of serializability but it
prone to deadlocks and lock contention \cite{berenson1995critique}.

In a \emph{multiversion concurrency control}, there can be multiple versions of
data items. This fundamentally changes the nature of both read and write
operations. Instead of updating an item in-place which would have to be
isolated, write operations create copies of existing versions and only modify
those copies. As a result, read operations are implicitly decoupled from
concurrent updates. That means, in particular, that a read operation may access
an item even when newer versions have been committed.

\todo[inline]{Does RCU fit in here somewhere?}
\todo[inline]{Does Shadow Paging fit in here somewhere?}
\todo[inline]{Mention that MVCC is based on COW and TS}
\todo[inline]{Mention that MVCC combines CC with recovery (implicit logging)}

Multiversioning can effectively isolate read operations from concurrent
operations without the need for locking. An important implication of this
circumstance is that read operations never wait for write operations and vice
versa. This is a significant advantage over single-version schemes especially in
applications where reading is much more frequent than writing. In fact, it has
been shown that many workloads are dominated by queries \cite{krueger2011fast,
andrei2017sap}. This is also reflected in the TATP benchmark which assumes 80 \%
of all transactions to be read-only \cite{larson2011high}. Write operations on
the other hand, are more complicated. Updating a version incorporates additional
overhead for allocating a new version and copying the original version before
modifying it.

\subsubsection{Visibility}

A central aspect of multiversioning is the concept of \emph{visibility}. Since
there may be multiple versions of a single data item, an operation must first
figure out to which version it should apply. For this purpose, the visibility
property is introduced. It determines which versions can be accessed by the
operations of a transaction. In other words, a version is \emph{visible} to a
transaction and its operations if and only if it satisfies the visibility
property. In general, visibility is defined as a predicate over timestamps of
transactions and versions. The concrete definition of the predicate is subject
to the respective MVCC protocol.

Now, when a transaction attempts to access an item, it would typically traverse
the versions of that item and determine for each version whether it is visible
to the transaction. Only if a version is visible to the issuing transaction, it
can be selected for reading or writing. The implementation of visibility is of
paramount importance for MVCC protocols and the desired isolation level
\cite{larson2011high}.

\subsubsection{Challenges}

The most promising feature of multiversioning is the non-blocking nature of read
operations due to the absence of in-place updates. However, there also important
problems that need to be addressed in order to leverage the merits of
multiversion concurrency control.

First, the maintenance of more than one version per item implies a significant
overhead in storage. Note that a version may not only contain payload but
additional data such as pointers to adjacent versions. A version may also be
required to hold certain metadata such as timestamps, further increasing the
overall memory footprint. This is relevant especially in areas where memory is
comparatively scarce as is in main-memory databases. However, not all versions
need to be retained. Instead, only the versions that are visible to at least one
transaction are needed. All other versions are considered \emph{stale} and can
be disposed of. This task is usually achieved by a designated garbage collection
mechanism. Although garbage collection may improve the overall memory footprint,
it is also known to have adverse effects on performance.

Second, whenever an item is accessed, the system first has to find a visible
version of the item. Accessing an item may therefore incur a significant runtime
overhead. The overhead mainly depends on the size of the history which, in
theory, is only bounded by the amount of available memory. Employing a garbage
collection mechanism can help by reducing the size of histories. Another
optimization would be to have a transaction keep track of all the versions it
references. This way, visibility would only have to be computed once for each
item.

\subsection{Snapshot Isolation}

The most widely used MVCC protocol to date is Snapshot Isolation
\cite{larson2011high, neumann2015fast}. Originally, Snapshot Isolation was
developed as a response to the insufficient definition of serializability in the
SQL standard \cite{berenson1995critique}. Since then, it has been deployed in
numerous databases and KVS \cite{cahill2009serializable, wu2017empirical}. This
section introduces the concept of Snapshot Isolation and its properties.

The core principle of snapshot isolation is that a transaction $t$ only sees a
private snapshot of the database as of when $t$ started. In this sense, the
notion of a snapshot comprises the set of the latest versions that have been
committed before $t$ was invoked. The key to this behaviour is the definition of
the visibility property.

\subsubsection{Visibility}

Each version $v$ stores two timestamps $begin_v$ and $end_v$, denoting when $v$
was created and when it was invalidated by an update or deletion, respectively.
The interval $[begin_v,  end_v]$ is called the \emph{lifetime} of $v$. Also,
when a transaction $t$ starts, it is given a timestamp $begin_t$ to capture when
$t$ started. In order to determine which version is visible to the operation of
a transaction, the concurrency control needs to test for each version whether
$t$ started during the lifetime of $v$. The latest valid version satisfying this
property is selected to be visible by the requesting transaction. More
precisely, the version seen by a transaction $t$ is

\[
\operatorname*{max}_{i \in \mathbb{N}}\, \{\, v_i\, |\, begin_{v_i} < begin_t < end_{v_i}\}.
\]

\subsubsection{Conflict Handling}

Concurrent updates by a transaction $t_2$ that happen after a transaction $t_1$
started, are not included in the snapshot of $t_1$ and are therefore invisible
to $t_1$. If however, $t_1$ decides to also update the same data item, then a
write-write conflict emerges. In this case, the \emph{first-committer-wins}
principle is applied and $t_1$ must abort as $t_2$ also modified the same item
and committed earlier \cite{berenson1995critique}. A popular variant of this
property is the equivalent \emph{first-updater-wins} principle
\cite{fekete2004read, larson2011high}. According to this property, a writer
fails immediately if it is not the first to attempt an update on a given
version, thus making the earlier transaction the first committer.

\todo[inline]{SI does not need validation with first-updater-wins}
\todo[inline]{first-updater wins is similar to pessimism but writes are rare + no waiting}

\subsubsection{Shortcomings}

Under snapshot isolation reads can always be satisfied, provided the requested
item exists. Note that even in the face of concurrent updates, a transaction
under snapshot isolation always sees the same items. This precludes inconsistent
reads, read skew, and phantoms. In addition, snapshot isolation does not allow
dirty reads, since snapshots only contain committed data. However, snapshot
isolation does not prevent all possible anomalies and is therefore not
serializable \cite{berenson1995critique, fekete2004read}. In particular, these
conflicts are write skew and non-serializable read-only transactions.

\paragraph{Write Skew}

The earliest known anomaly of Snapshot Isolation is write skew. The reason for
its occurrence is that under snapshot isolation a transaction does not see
modifications to versions that have been read during the transaction.

Imagine two transactions $t_1, t_2$ reading two data items $x, y$ constrained by
a predicate $C$. Next, $t_1$ updates $x$ and finds that $C(x^{*}, y)$ still
holds true. At this point $t_2$ is unaware that $x$ has been modified and
updates $y$. Since $t_2$ does not see the modifications of $t_1$, it also
evaluates $C(x, y^{*})$ to be true. Finally, both transactions may commit even
if $C$ is now violated because none observed the others changes (see Figure
\ref{fig:write_skew}). Note that no write-write conflict occurs as both updated
items are distinct. In fact, write skew is said to occur if read sets overlap
while write sets are distinct.

\begin{figure}[!h]
    \centering
    \begin{tabular}{r c c c c c}
        $t_1:$ & $r(x,y)$ &          & $w(x)$ &        & $c$ \\
        $t_2:$ &          & $r(x,y)$ &        & $w(y)$ & $c$ \\
    \end{tabular}
    \caption{Write skew due to transactions $t_1, t_2$ not seeing each others changes.}
    \label{fig:write_skew}
\end{figure}

In the field, write skew has been countered by inducing artificial write
conflicts between transactions that are expected to exhibit write skew
\cite{fekete2005making}.

\paragraph{Non-Serializable Read-Only}

Another anomaly was discovered almost 10 years after the introduction of
snapshot isolation. It proved, contrary to common understanding, that even
read-only transactions may not always be serializable \cite{fekete2004read}. The
proof consists of a schedule of three transactions with one being read-only. The
schedule is constructed in a way that at most two but never all three
transactions can execute serializably.

Suppose a pair of data items $x = 0$ and $y = 0$ and transactions $t_1, t_2,
t_{RO}$. Further, let $t_1$ compute $y = y - 10$ and also subtract one if $x + y <
0$, while $t_2$ sets $x = x + 20$. The schedule given in Figure
\ref{fig:bad_read_only} shows that while $t_1$ is the first transaction to start
execution, both $t_2$ and $t_{RO}$ start and commit sequentially before $t_1$
issues its update on $y$. This means that $t_{RO}$ will see the update of $x$ by
$t_2$ while $t_1$ does not. According to the output of $t_{RO}$ ($x = 20$, $y =
0$), a serializable schedule would require $t_1$ to have been executed after
both $t_2$ and $t_{RO}$. This however, is not possible since $t_1$ would have
seen the update of $t_2$ and no penalty would have been applied as $20 - 10 \geq
0$. Likewise, in order for $t_1$ to yield $y = -11$, it would have had to be
executed before $t_2$ (and $t_{RO}$) which however is not consistent with the
output of $t_{RO}$. In fact, the output of $t_{RO}$ corresponds the the exact
opposite serial ordering as do those of $t_1$ and $t_2$.

\begin{figure}[h!]
    \centering
    \begin{tabular}{r c c c c}
    $t_1:$    & $r(x,y)$ &                   &              & $w(y)\, c$ \\
    $t_2:$    &          & $r(x)\, w(x)\, c$ &              &            \\
    $t_{RO}:$ &          &                   & $r(x,y)\, c$ &
    \end{tabular}
    \caption{Transaction $t_{RO}$ is read-only but not serializable.}
    \label{fig:bad_read_only}
\end{figure}

Both symptoms can be attributed to the fact, that snapshot isolation fails to
observe read-write conflicts. When a transaction requests an item, it reads the
latest version that has been committed before the transaction started. This way,
a transaction always reads the same version even if a newer version has been
committed concurrently. This relieves the system from locking a version when
accessing it. The downside is that every transaction is effectively isolated
from any concurrent modifications. As a consequence, transactions may
successfully commit even if one or more versions they have read has been updated
in the meantime. All anomalies known to be emitted by snapshot isolation can be
reduced to read-write conflicts.

\subsection{Serializable MVCC}

The previous section introduced the MVCC protocol Snapshot Isolation. SI
provides affordable isolation but fails to preclude all non-serializable
schedules such as write skew. Nevertheless, SI is the most widely adopted MVCC
implementation \cite{larson2011high, bailey2013exploring, neumann2015fast}. In
some cases, serializing alternatives are available but disabled by default. This
policy is often motivated by significant performance benefits compared to
strictly serializing concurrency control \cite{cahill2009serializable}. Others
argue that SI anomalies may be negligible as even renowned ACID-compliant
benchmarks such as TPC-C do not expose them \cite{fekete2005making}. However,
the need for data integrity should not be based on benchmarks failing to prove
it. Therefore, there are efforts to overcome the weaknesses of SI and provide
strong consistency without falling back to pessimistic approaches. This section
presents an overview on approaches to making serializing MVCC affordable.

The main reason for non-serializable schedules in SI is that it cannot detect
read-write conflicts. While SI keeps track of each transactions' updates for
installment on commit, reads are not tracked. Therefore, a naive approach to
achieve serializable schedules with SI is to track the read operations of each
transaction. In doing so, read-write conflicts can be detected during validation
by looking at the timestamps of all versions read. If at least one of these
versions has been invalidated after the transaction started, then a read-write
conflict has emerged. Unless the conflicting updater is still running, the
reading transaction must abort. This method is both simple and effective but
introduces significant overhead. Note that traditional SI does not perform any
validation if first-updater-wins is used for precluding write-write conflicts.
The additional overhead especially affects read-mostly transactions which is
undesirable in read-dominated environments. Since the latter is where SI has
been especially successful, tracking reads followed by validation is often
stated as prohibitively expensive \cite{cahill2009serializable}. Lacking viable
alternatives, research interests primarily focus on mitigating the footprint of
read tracking and validation.

\subsubsection{Exploiting Query Languages}

Several authors have proposed to detect conflicts from query language statements
\cite{fekete2005making, faleiro2015rethinking, neumann2015fast}. For instance, a
common strategy to prevent write skew in SI is to inject detectable conflicts
whenever the required access patterns are detected \cite{fekete2005making}.
Others have found efficient ways to determine whether an item is included in a
range query, thus improving validation time \cite{neumann2015fast}. Although
intriguing, these approaches cannot be applied to KVS since they operate through
ad-hoc queries instead of query languages.

\subsubsection{Reducing Contention}

A major challenge for high-performance implementations of MVCC and SI, in
particular, is that important aspects such as timestamping or validation are
often centralized which can cause considerable contention. Therefore, a
substantial amount of research is dedicated to providing protocols in the spirit
of SI but lower contention. Note that, in contrast to locking, contention for
MVCC denotes much smaller intervals of a transaction's lifetime.

A major bottleneck in MVCC implementations is validation \cite{tu2013speedy,
bailey2013exploring, ding2015centiman, faleiro2015rethinking,
wang2017efficiently, zhou2017posterior}. The reason is that, validation
typically requires mutual exclusion since concurrent updates to items from the
read set could falsify the validation result. As a result, validation does not
scale, thus inhibiting transaction throughput. Therefore, many authors propose
protocols featuring concurrent validation \cite{bailey2013exploring,
ding2015centiman, faleiro2015rethinking, wang2017efficiently}. Parallel
validation has already been proposed in the early days of MVCC
\cite{kung1981optimistic}, but received new interest lately.

Another point of contention is the assignment of timestamps. A typical SI
implementation requires multiple timestamps for both versions and transactions.
However, most implementations rely on a global assignment policy which
introduces a significant contention on multi-core systems \cite{tu2013speedy,
zhou2017posterior}. First, concurrency is reduced as mutual exclusion is
required to guarantee strictly monotone timestamps. Second, the CPU must be
informed that changes to the cached timestamp counter must be globally visible.
This is usually done with fences which can further reduce performance. In
response, some authors have proposed protocols featuring decentralized timestamp
assignment \cite{tu2013speedy, zhou2017posterior}. Further sources of contention
addressed in these works are transaction id assignment and shared memory access,
in general.

\todo[inline]{Überleitung zu CC in KVS für NVRAM}
\todo[inline]{Hybride Ansätze: Optimistic + Locking (mao2012cache)}
