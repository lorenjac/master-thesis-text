Concurrency is a major building block for scalable transaction processing. It
enables higher transaction throughput and resource utilization compared to
serial processing. On the downside, concurrent schedules are subject to
potential conflicts that may result in data corruption. However, it is not
sufficient to provide mutual exclusion for individual operations within a
transaction. In other words, the scope in which isolation is required spans
beyond critical sections. Therefore, a dedicated concurrency control is required
to ensure isolation. Unfortunately, concurrency controls do not come without overhead which is why, in most cases, a compromise between isolation and performance must be found.

This section deals with concurrency control strategies and outlines the state of
the art with a focus on optimistic approaches, in particular multiversion
concurrency control.

\subsection{Strategies}

There are two fundamental approaches to the design of concurrency controls:
\emph{pessimistic} and \emph{optimistic} protocols \cite{kung1981optimistic,
larson2011high, sadoghi2014reducing}. The distinction is based on whether
conflicts are assumed to be frequent or infrequent. Still, both strategies share
their intent to prevent conflicts from manifesting into inconsistencies.

\paragraph{Pessimistic Concurrency Control}

A pessimistic concurrency control assumes that conflicts are frequent and
strives to prevent conflicts before they can even emerge. To this end,
pessimistic control mechanisms employ some form of exclusive ownership. That
means, a transaction must acquire the ownership of all data items it wishes to
access. If the resource acquisition succeeds, then the transaction can freely
operate on the temporarily owned data. Only when that transaction terminates,
will this ownership be released. If a transaction fails to claim the exclusive
ownership on its data then it has to wait until the required ownership is
granted or abort.

Pessimistic concurrency control is usually implemented with locks. Locking
provides a solid mechanism to ensure serializability and most database systems
implement it \cite{kung1981optimistic, berenson1995critique, larson2011high}.

Despite their prevalence, pessimistic concurrency controls have notable
drawbacks. Locking-based concurrency controls are prone to \emph{deadlocks}
\cite{bernstein1981concurrency, kung1981optimistic}. In order to prevent
deadlocks, they must be detected and resolved which introduces runtime overhead.
Unfortunately, there is no general-purpose locking protocol that precludes
deadlocks \cite{kung1981optimistic}. Another problem is \emph{lock contention}
which occurs when a large portion of concurrent threads in a system compete for
a single shared resource \cite{berenson1995critique, sadoghi2014reducing}. Since
only one transaction can manage to acquire ownership, all remaining transactions
are left waiting for it to complete and contend again. As a result, only one
transaction is executed at a time, thus defeating the purpose of concurrency.

\paragraph{Optimistic Concurrency Control}

Optimistic concurrency controls form the opposite of pessimistic control
schemes. Instead of preventing conflicts altogether, optimistic control schemes
do not enforce consistency until a transaction commits. Only when a transaction
commits, the concurrency control starts to check for violations, a step called
\emph{validation}. If no conflict is detected, then the transaction may
commit, otherwise, it must abort.

Optimistic concurrency control protocols rely on copy-on-write (CoW) and
timestamp ordering to synchronize data races of competing transactions
\cite{bernstein1981concurrency, kung1981optimistic}. While readers can access
data without further means of synchronization, writers apply their modifications
only on copies of the original data. Apart from short-duration locks for
critical sections, this approach does not require locking and is therefore not
subject to deadlocks or lock contention. A signature property is that readers
never block which means that both readers and writers never wait for other
readers. Due to its nature, optimistic concurrency control is also called
multiversion concurrency control or multiversioning
\cite{bernstein1983multiversion}.

There are, however, disadvantages to this type of concurrency control. While
validation is necessary to ensure that a transaction is not involved in data
conflicts, it can also introduce a significant overhead. First, even when there
are no conflicts, validation is conducted nevertheless. Second, validation
usually requires certain meta data about the operations within a transaction. As
a result, validation complexity scales in size of meta data required to
determine conflict freedom. Another drawback is that aborting a conflicting
transaction means that its entire progress is discarded. In this case, valuable
computing resources have been wasted. This is especially true for update
operations as they involve potentially expensive copy operations.

Despite some drawbacks, optimistic concurrency control has found wide adoption
especially in domains where reads are dominant and conflicts are known to be
infrequent or negligible \cite{carey1986performance, larson2011high,
wu2017empirical}. The scenario of read-dominated workloads has been shown to
apply more often than not \cite{krueger2011fast, andrei2017sap}. Given its
promising properties, optimistic concurrency control is discussed in more depth
in the subsequent sections.

\subsection{Multiversion Concurrency Control}

Multiversioning is a popular concurrency control mechanism. While originating in
distributed systems research \cite{reed1978naming}, it was soon considered a
promising alternative to traditional locking-based approaches
\cite{kung1981optimistic, bernstein1983multiversion, carey1983multiple,
hadzilacos1986algorithmic, carey1986performance}.

Today, MVCC forms the
foundation of many commercial products \cite{larson2011high}. It is also featured in main-memory databases
such as SAP HANA and recent research in non-volatile main memory databases
\cite{lee2013high, schwalb2015efficient, schwalb2016hyrise, oukid2014sofort}.
However, most implementations of MVCC, such as snapshot isolation, do not
guarantee full serializability and are thus prone to inconsistencies
\cite{neumann2015fast, berenson1995critique}. Although recent research has shown
ways to achieve serializability with MVCC, implementations are hesitant to
follow \cite{larson2011high}.

MVCC has not only been applied in traditional DBMS but also in main-memory
databases. With emerging non-volatile memory that is both affordable and little
slower than conventional DRAM, new problems for MVCC such as durability,
recovery or access time arise \cite{bailey2011operating, larson2011high,
oukid2014sofort, schwalb2016hyrise}. In order to fully leverage the emerging
memory technology, it is important to assess opportunities and issues of MVCC
especially when serializability is required.

Multiversioning, which is also referred to as multiversion concurrency control
(MVCC), is a popular concurrency control method. It has been implemented in both
commercial and non-commercial transaction-processing systems, ranging from
general-purpose database systems to high-performance in-memory databases.
Initially designed as a solution to concurrency control for distributed and
nested transactions \cite{reed1978naming}, MVCC has been widely adopted as an
alternative to conventional locking-based concurrency schemes. This move was
motivated in overcoming typical problems associated with locking, such as
deadlocks and lock contention.

A \emph{version} is a snapshot of a particular data item within a database. In
traditional concurrency schemes, there is exactly one version of each item.
These are also referred to as \emph{single-version} concurrency schemes. If a
transaction issues an update to this version, then it is performed in-place. In
order to ensure isolation though, a transaction has to be protected against
concurrent reading or writing. This is usually achieved by having each
transaction acquire locks on its data which are only released once the
transaction terminates. This approach is very effective in that it can provide
the highest isolation level of serializability as in DB2 or MySQL
\cite{berenson1995critique}. However, it also has some notable drawbacks such as
deadlocks and lock contention.

In a \emph{multiversion} concurrency scheme, multiple versions of an item can be
maintained. This fundamentally changes the nature of both read and write
operations. Instead of updating an item in-place which would have to be
isolated, write operations create new versions by modifying copies of existing
versions. Also, since multiple versions are kept, read operations do not need to
access the latest version and may instead select an earlier version. This means
that not only may a transaction read an item which is currently being updated
but it is also able to continue accessing the same item even if there exists a
newer version.

This scheme can effectively isolate read operations from concurrent operations
without the need for locking. An important implication of this property is that
read operations never wait for write operations and vice versa. This is a
significant advantage over single-version schemes especially in applications
where reading is much more frequent than writing (e.g. OLAP). In fact, it has
been shown that even in OLTP systems \emph{typical} workloads are dominated by
queries \cite{krueger2011fast, andrei2017sap}. This is also reflected in the
TATP benchmark for OLTP systems which assumes 80 \% of all transactions to be
read-only \cite{larson2011high, neumann2015fast, oukid2015instant}.

This is different with write operations. Updating a version incorporates
additional overhead for allocating a new version and copying the original
version before modifying it. Also concurrent writes to an item may need to be
addressed to prevent write-write conflicts.

On the other hand, the presence of multiple versions also implies that whenever
a transaction issues a operation on a data item, it first has to determine which
version the operation applies to. This is done by traversing the version
\emph{history} of the item and testing for each version its \emph{visibility}. A
version is said to be \emph{visible} if and only if it satisfies a well-defined
predicate. This predicate can be formulated over operations or transactions.
Only if a version is visible it can be selected for reading or writing,
respectively. The implementation of the selection method depends both on the
type of concurrency control, that is optimistic or pessimistic, and the desired
isolation level.

The most promising feature of multiversioning is the non-blocking nature of read
operations due to the absence of in-place updates. However, there also important
problems that need to be addressed in order to leverage the merits of
multiversion concurrency control.

First, the maintenance of multiple versions per item implies a significant
overhead in storage. Note that a version may not only contain payload but
additional data such as handles pointing to adjacent versions. A version may
also be required to hold certain metadata such as timestamps, further increasing
the overall memory footprint. This may be relevant especially in areas where
memory is comparatively scarce as is in main-memory databases. However, not all
versions need to be retained. Instead, only the versions that are visible to at
least one transaction are needed. All other versions can be considered
\emph{stale} and be disposed. This task is usually achieved by a designated
garbage collection mechanism. Although garbage collection may improve the
overall memory footprint it is also known to have adverse effects on
performance. Another drawback is that in the presence of many long-running
transactions, versions are less likely to go stale and cannot be released.

Second, whenever an item is accessed, the system first has to find a visible
version of the item. Accessing an item may therefore incur a significant runtime
overhead. The overhead mainly depends on the size of the history and is thus
bounded by the longest available history. Employing a garbage collection
mechanism can reduce the size of version histories thus improving the upper
bound of the visibility test. Another optimization would be to have a
transaction keep track of all the versions it references. This way, visibility
would only have to be computed once for each item.

\subsection{Snapshot Isolation}

Snapshot Isolation \cite{berenson1995critique} is a popular MVCC algorithm that
has been deployed in numerous databases such as the ORACLE RDBMS, Microsoft SQL
Server, PostgreSQL and BerkeleyDB \cite{cahill2009serializable}. It is based on
timestamping and does not require locking. This section introduces the concept
of Snapshot Isolation and its properties.

The core princple of snapshot isolation is that a transaction $T$ only sees a
private snapshot of the database as of when $T$ started. In this sense, the
notion of a snapshot comprises the set of the latest versions of each data item
that have been committed before $T$ was invoked.

Concurrent updates by a transaction $t_2$ that happen after a transaction $t_1$
started, are not included in the snapshot of $t_1$ and are thus invisible to
$t_1$. If however, $t_1$ decides to also update the same data item, then a
write-write conflict emerges. In this case, the \textit{first-committer-wins}
principle is applied and $t_1$ must abort as $t_2$ also modified the same item
and committed earlier. A popular variant of this property is the equivalent
\textit{first-updater-wins} principle \cite{fekete2004read, larson2011high}.
According to this property, a writer fails immediately if it is not the first to
update a given version, thereby making the updating transaction the first
committer.

Each version keeps a \textit{begin timestamp} and an \textit{end timestamp},
that store the point in time when it was created and when it was invalidated by
deletion or an update. Also, when starting, every transaction is given a
timestamp denoting its start time. In order to determine which version is
visible to the operation of a transaction, the concurrency control tests for
each version whether the transaction begin falls between the begin and the end
timestamp of the version. The latest valid version satisfying this property is
selected to be visible by the requesting transaction.

Under snapshot isolation reads can always be satisfied, provided the requested
item exists. Note that even in the face of concurrent updates, a transaction
always reads the same data items. This precludes inconsistent reads, read skew
and phantoms. In addition, snapshot isolation does not allow dirty reads, since
snapshots only contain committed data. However, snapshot isolation does not
prevent all possible anomalies and is therefore not serializable
\cite{berenson1995critique}. These anomalies, namely write skew and another
similar anomaly will be discussed below.

\paragraph{Write Skew}

A classic example in this regard is write skew. The reason for its occurrence is
that under snapshot isolation a transaction does not see modifications to
versions that have been read during the transaction.

Imagine two transactions $t_1, t_2$ reading two data items $x, y$ constrained by
a predicate $C$. Next, $t_1$ updates $x$ and finds that $C(x^{*}, y)$ still
holds true. At this point $t_2$ is unaware that $x$ has been modified and
updates $y$. Since $t_2$ does not see the modifications of $t_1$, it also
evaluates $C(x, y^{*})$ to be true. Finally, both transactions may commit even
if $C$ is now violated because none observed the others changes (see figure
\ref{fig:write_skew}). Note that no write-write conflict occurs as both updated
items are distinct. In fact, write skew is said to occur if read sets overlap
while write sets are distinct.

\begin{figure}
    \centering
    \[
        r_1[x,y]\; r_2[x,y]\; w_1[x]\; w_2[y]\; c_1\; c_2\;
    \]
    \caption{Write skew due to transactions $t_1, t_2$ not seeing each others changes.}
    \label{fig:write_skew}
\end{figure}

In the field, write skew has been countered by inducing artificial write
conflicts between transactions that are expected to exhibit write skew
\cite{fekete2005making}.

\paragraph{Non-Serializable Read-Only}

Another anomaly was discovered almost 10 years after the introduction of
snapshot isolation. It proved, contrary to common understanding, that even
read-only transactions may not always be serializable \cite{fekete2004read}. The
proof consists of a schedule of three transactions with one being read-only. The
schedule is constructed in a way that at most two but never all three
transactions can execute serializably.

Suppose a pair of data items $x = 0$ and $y = 0$ and transactions $t_1, t_2,
t_{RO}$. Further, let $t_1$ compute $y = y - 10$ and also subtract one if $x + y <
0$, while $t_2$ sets $x = x + 20$. The schedule given in figure
\ref{fig:bad_read_only} shows that while $t_1$ is the first transaction to start
execution, both $t_2$ and $t_{RO}$ start and commit sequentially before $t_1$
issues its update on $y$. This means that $t_{RO}$ will see the update of $x$ by
$t_2$ while $t_1$ does not. According to the output of $t_{RO}$ ($x = 20$, $y =
0$), a serializable schedule would require $t_1$ to have been executed after
both $t_2$ and $t_{RO}$. This however, is not possible since $t_1$ would have
seen the update of $t_2$ and no penalty would have been applied as $20 - 10 \geq
0$. Likewise, in order for $t_1$ to yield $y = -11$, it would have had to be
executed before $t_2$ (and $t_{RO}$) which however is not consistent with the
output of $t_{RO}$. In fact, the output of $t_{RO}$ corresponds the the exact
opposite serial ordering as do those of $t_1$ and $t_2$.

\begin{figure}[h!]
    \centering
    \[
        r_1[x,y]\; r_2[x]\; w_2[x]\; c_2\; r_3[x,y]\; c_3\; w_2[y]\; c_2\;
    \]
    \caption{Transaction $t_3$ is read-only but not serializable.}
    \label{fig:bad_read_only}
\end{figure}

Both symptoms can be related to the fact, that snapshot isolation fails to
observe read-write conflicts. When a transaction requests an item, it reads the
latest version that has been committed before the transaction started. This way,
a transaction always reads the same version even if a newer version has been
committed concurrently. The downside is that every transaction is effectively
isolated from any concurrent modifications. As a consequence, transactions may
successfully commit even if one or more versions they have read has been updated
in the meantime. All anomalies known to be emitted by snapshot isolation can be
reduced to read-write conflicts.

\subsection{Serializable MVCC}

The previous chapter introduced the renowned MVCC algorithm of snapshot
isolation. Even though snapshot isolation permits non-serializable schedules
that can lead to inconsistent data, it is still and to this date the most widely
adopted MVCC algorithm \cite{cahill2009serializable, larson2011high,
sikka2012efficient, neumann2015fast}. Notable anomalies resulting from
non-serializable schedules are write skew and non-serializable read-only
transactions. In fact, most systems do not provide serializable isolation
degrees by default, even if supported. This is often motivated by significantly
improved performance. Others argue that anomalies may be negligible as even the
renowned ACID-compliant TPC-C benchmark does not exhibit them
\cite{fekete2005making}.

In order to still achieve serializable multiversion concurrency controls some
advanced methods have been proposed \cite{fekete2005making,
cahill2009serializable, neumann2015fast}. The examined approaches can be broken
down as follows:

\begin{itemize}
    \item Keep Snapshot Isolation but modify transactional database programs
    \item Extend Snapshot Isolation with read-write conflict detection
    \item Replace Snapshot Isolation with a custom implementation
\end{itemize}
