Evaluating database systems or database concepts in general is a complicated
topic and there are even additional issues in this particular instance.
Essentially though, the evaluation of Midas is facing two major challenges:

\begin{enumerate}
    \item \ac{NVRAM} is not part of the system configuration
    \item Established database benchmark software cannot be used
\end{enumerate}

Promising \ac{NVRAM} technologies such as \ac{PCM} or 3DXPoint are not yet
commercially available as \acp{DIMM}. Therefore, the evaluation has to be
carried out on volatile \ac{DRAM} which does not account for the greater access
latencies of \ac{NVRAM}. In order to still obtain meaningful results, it is
possible to enforce custom latencies by modifying the system BIOS as in
\cite{dulloor2014system, oukid2015instant} or by reprogramming the \ac{DIMM}
microcode \cite{schwalb2016hyrise}. Some authors, however, have decided not to
emulate latencies \cite{bailey2013exploring, zhou2016nvht}. Arguments include
that any emulated latency would be inherently inaccurate as final \ac{NVRAM}
parameters are yet unknown. Further, they argue that it is safe to assume that
the current latency gap of up to 10\% can be eliminated as technologies advance.
The issue of emulating \ac{NVRAM} is addressed in
Section~\ref{ch:eval-sysconfig}.

In general, database systems are very complex software systems with a wide range
of mutually differing feature sets, technologies, and optimization goals. As a
result, it is very hard to identify use cases or workloads that are meaningful
for all systems under evaluation. Ultimately, it is difficult to compare the
performance of these systems against each other. Therefore, vendors usually rely
on established benchmark software such as TPC \cite{schwalb2016hyrise} or TATP
\cite{oukid2015instant} as a reference. However, these systems require all their
test candidates to provide a query language front end which is used to run their
benchmark routines. Since neither \ac{KVS} in this evaluation supports a query
language, none of these benchmark suites can be used. As a result, custom
benchmark routines must be developed as is done in similar works
\cite{bailey2013exploring, zhou2016nvht}. This issue is discussed in
Section~\ref{ch:eval-ttp}.
